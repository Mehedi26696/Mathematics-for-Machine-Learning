{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e18672-d7da-4a53-8ee0-82edec780288",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Evaluation metrics are essential tools for assessing the performance of statistical and machine learning models. They provide quantitative measures that help us understand how well a model is performing and where improvements can be made. In both classification and regression tasks, selecting appropriate evaluation metrics is crucial for model selection, tuning, and validation.\n",
    "\n",
    "## Classification Metrics\n",
    "\n",
    "In classification tasks, the goal is to assign input data into predefined categories or classes. Evaluating classification models requires metrics that capture the correctness and reliability of the predictions.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a tabular representation of the performance of a classification model. It compares the actual target values with those predicted by the model, providing a breakdown of correct and incorrect predictions. The confusion matrix for a binary classification problem is typically structured as follows:\n",
    "\n",
    "|                      | **Predicted Positive** | **Predicted Negative** |\n",
    "|----------------------|------------------------|------------------------|\n",
    "| **Actual Positive**  | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative**  | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "- A **true positive (TP)** occurs when the model correctly predicts a positive class.\n",
    "- A **true negative (TN)** happens when the model correctly predicts a negative class.\n",
    "- A **false positive (FP)** means the model incorrectly predicts a positive class, which is also known as a Type I error.\n",
    "- A **false negative (FN)** refers to the model incorrectly predicting a negative class, known as a Type II error.\n",
    "\n",
    "### Key Metrics Derived from the Confusion Matrix\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "**Accuracy** measures the proportion of total correct predictions made by the model:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "While accuracy is intuitive, it can be misleading in imbalanced datasets where one class significantly outnumbers the other.\n",
    "\n",
    "#### Precision\n",
    "\n",
    "**Precision** quantifies the correctness of positive predictions made by the model:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "High precision indicates a low rate of false positives.\n",
    "\n",
    "#### Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "**Recall** measures the model's ability to identify all actual positive cases:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "High recall indicates a low rate of false negatives.\n",
    "\n",
    "#### Specificity (True Negative Rate)\n",
    "\n",
    "**Specificity** assesses the model's ability to identify all actual negative cases:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "High specificity indicates a low rate of false positives.\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "The **F1 Score** is the harmonic mean of precision and recall, providing a balance between the two metrics:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "The F1 Score is particularly useful when dealing with imbalanced classes.\n",
    "\n",
    "### Intuitive Explanation of Precision and Recall\n",
    "\n",
    "Imagine you are a librarian tasked with retrieving science fiction books from a large library that contains various genres.\n",
    "\n",
    "**Precision**: \n",
    "\n",
    "- Of all the books you retrieved, how many are actually science fiction?\n",
    "- If you retrieved 50 books and 40 of them are science fiction, your precision is $\\frac{40}{50} = 0.8$ or 80%.\n",
    "  \n",
    "**Recall**: \n",
    "\n",
    "- Of all the science fiction books in the library, how many did you successfully retrieve?\n",
    "- If there are 100 science fiction books in total and you retrieved 40, your recall is $\\frac{40}{100} = 0.4$ or 40%.\n",
    "\n",
    "High precision means that most of the books you picked are relevant (few false positives), while high recall means you found most of the relevant books (few false negatives).\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Suppose we have a binary classification problem with the following confusion matrix:\n",
    "\n",
    "|                      | **Predicted Positive** | **Predicted Negative** | **Total** |\n",
    "|----------------------|------------------------|------------------------|-----------|\n",
    "| **Actual Positive**  | TP = 70                | FN = 30                | 100       |\n",
    "| **Actual Negative**  | FP = 20                | TN = 80                | 100       |\n",
    "| **Total**            | 90                     | 110                    | 200       |\n",
    "\n",
    "**Accuracy**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{70 + 80}{200} = \\frac{150}{200} = 0.75 \\text{ or } 75\\%\n",
    "$$\n",
    "\n",
    "**Precision**:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{70}{70 + 20} = \\frac{70}{90} \\approx 0.778 \\text{ or } 77.8\\%\n",
    "$$\n",
    "\n",
    "**Recall**:\n",
    " \n",
    "$$\n",
    "\\text{Recall} = \\frac{70}{70 + 30} = \\frac{70}{100} = 0.7 \\text{ or } 70\\%\n",
    "$$\n",
    "\n",
    "**Specificity**:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.8 \\text{ or } 80\\%\n",
    "$$\n",
    "\n",
    "**F1 Score**:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{0.778 \\times 0.7}{0.778 + 0.7} \\approx 2 \\times \\frac{0.5446}{1.478} \\approx 0.736 \\text{ or } 73.6\\%\n",
    "$$\n",
    "\n",
    "### Receiver Operating Characteristic (ROC) Curve and AUC\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC) curve** plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various threshold settings. It provides a comprehensive view of the model's performance across all classification thresholds.\n",
    "\n",
    "**True Positive Rate (TPR)**:\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "**False Positive Rate (FPR)**:\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "$$\n",
    "\n",
    "The **Area Under the ROC Curve (AUC-ROC)** is a single scalar value summarizing the model's ability to discriminate between positive and negative classes. An AUC of:\n",
    "\n",
    "- **1.0**: Perfect classifier.\n",
    "- **0.5**: No discriminative ability (equivalent to random guessing).\n",
    "- **Less than 0.5**: Worse than random guessing (model may be inverted).\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- A higher AUC indicates better model performance.\n",
    "- The ROC curve helps in selecting the optimal threshold that balances sensitivity and specificity.\n",
    "\n",
    "### Precision-Recall Curve and AUC\n",
    "\n",
    "In cases of imbalanced datasets, the **Precision-Recall (PR) curve** is more informative than the ROC curve. It plots Precision against Recall for different thresholds.\n",
    "\n",
    "- The **Area Under the PR Curve (AUC-PR)** provides a summary metric that is sensitive to class imbalance.\n",
    "- A higher area under the PR curve indicates better performance in identifying the positive class.\n",
    "\n",
    "## Regression Metrics\n",
    "\n",
    "Evaluation metrics are crucial in assessing the performance of regression models, which predict a continuous outcome variable based on one or more predictor variables. These metrics quantify the discrepancy between the predicted values and the actual observed values, providing insights into the accuracy and reliability of the model.\n",
    "\n",
    "### Evaluating Regression Models\n",
    "\n",
    "In regression analysis, the goal is to build a model that accurately predicts the dependent variable $y$ from one or more independent variables $x $. After fitting a regression model, it is essential to assess its performance using appropriate evaluation metrics. The most commonly used regression metrics include:\n",
    "\n",
    "- **Mean Absolute Error (MAE)**\n",
    "- **Mean Squared Error (MSE)**\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "- **Coefficient of Determination ($R^2$)**\n",
    "\n",
    "These metrics provide different perspectives on the model's predictive capabilities and can be used to compare different models or to tune model parameters.\n",
    "\n",
    "### Visualizing Regression Performance\n",
    "\n",
    "Consider a dataset where we have generated data points from a sine wave with added noise. We fit a linear regression model to this data to predict $y$ based on $x $. The following plot illustrates the data and the fitted regression line:\n",
    "\n",
    "<img src=\"m1.png\">\n",
    "\n",
    "- **Blue Points**: Actual data points (sine wave with noise)\n",
    "- **Red Line**: Prediction from the linear regression model\n",
    "- **Gray Dotted Lines**: Errors (residuals) for each data point, connecting the actual value to the predicted value\n",
    "\n",
    "This visualization helps to understand how well the linear model captures the underlying pattern in the data and highlights the discrepancies between the predictions and actual values.\n",
    "\n",
    "### Regression Evaluation Metrics\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "The **Mean Absolute Error (MAE)** measures the average magnitude of the errors in a set of predictions, without considering their direction. It is the average over the test sample of the absolute differences between predicted and actual observations.\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ is the actual value of the dependent variable for the $i $-th observation\n",
    "- $\\hat{y}_i$ is the predicted value for the $i $-th observation\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- MAE is in the same units as the dependent variable.\n",
    "- It provides a linear score, meaning all individual differences are weighted equally.\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- MAE is robust to outliers compared to MSE because it does not square the errors.\n",
    "- It gives a straightforward measure of average error magnitude.\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "The **Mean Squared Error (MSE)** measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- MSE penalizes larger errors more severely due to squaring the errors.\n",
    "- It is sensitive to outliers since larger errors have a disproportionate effect.\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- MSE is always non-negative; values closer to zero indicate a better fit.\n",
    "- It is in squared units of the dependent variable, which can make interpretation less intuitive.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "The **Root Mean Squared Error (RMSE)** is the square root of the MSE. It brings the error metric back to the same units as the dependent variable, making interpretation more intuitive.\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2 }\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- RMSE represents the standard deviation of the residuals (prediction errors).\n",
    "- It indicates how concentrated the data is around the line of best fit.\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- Like MSE, RMSE is sensitive to outliers due to the squaring of errors.\n",
    "- RMSE is in the same units as the dependent variable.\n",
    "\n",
    "#### Coefficient of Determination ($R^2$)\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "The **Coefficient of Determination**, denoted as $R^2$, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{ \\text{SS}_{\\text{res}} }{ \\text{SS}_{\\text{tot}} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 - \\frac{ \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2 }{ \\sum_{i=1}^{n} ( y_i - \\bar{y} )^2 }\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\text{SS}_{\\text{res}}$ is the residual sum of squares (sum of squared residuals)\n",
    "- $\\text{SS}_{\\text{tot}}$ is the total sum of squares (sum of squared deviations from the mean)\n",
    "- $\\bar{y}$ is the mean of the observed data\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- $R^2$ values range from 0 to 1.\n",
    "- An $R^2$ of 1 indicates that the regression predictions perfectly fit the data.\n",
    "- An $R^2$ of 0 indicates that the model does not explain any of the variability of the response data around its mean.\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- $R^2$ can also be negative in cases where the model fits the data worse than a horizontal line (mean of the data).\n",
    "- Higher $R^2$ values indicate a better fit to the data.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- $R^2$ alone does not indicate whether a regression model is adequate. It is possible to have a high $R^2$ value for a model that is inappropriate.\n",
    "- It does not indicate whether independent variables are a cause of the dependent variable.\n",
    "- $R^2$ does not account for the number of predictors in the model. **Adjusted $R^2$** is used when comparing models with different numbers of predictors.\n",
    "\n",
    "#### Adjusted $R^2$\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "**Adjusted $R^2$** adjusts the $R^2$ value based on the number of predictors in the model relative to the number of data points. It penalizes the addition of unnecessary predictors to the model.\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{ (1 - R^2)(n - 1) }{ n - p - 1 } \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $p$ is the number of predictors (independent variables)\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- Adjusted $R^2$ increases only if the new predictor improves the model more than would be expected by chance.\n",
    "- It can be used for model selection, especially when comparing models with different numbers of predictors.\n",
    "\n",
    "### Comparing Regression Metrics\n",
    "\n",
    "#### Sensitivity to Outliers\n",
    "\n",
    "- The **MAE** is less sensitive to outliers than MSE and RMSE because it does not involve squaring the errors.\n",
    "- The **MSE/RMSE** are more sensitive to outliers as squaring the errors amplifies the impact of larger errors.\n",
    "\n",
    "#### Units and Interpretability\n",
    "\n",
    "- Both **MAE** and **RMSE** are expressed in the same units as the dependent variable, making their interpretation straightforward.\n",
    "- The **MSE**, however, is in squared units of the dependent variable, which can make interpretation more difficult.\n",
    "\n",
    "#### Usage in Model Evaluation\n",
    "\n",
    "- The **MAE** is ideal when all errors are considered equally important, and outliers are not a significant concern.\n",
    "- The **MSE/RMSE** are preferable when large errors are highly undesirable, as they penalize these more severely.\n",
    "- The **$R^2$** value provides insight into how well the model replicates the observed outcomes compared to the mean of the dependent variable.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Let's consider a small dataset:\n",
    "\n",
    "| $i $ | $x_i$ | $y_i$ | $\\hat{y}_i$ |\n",
    "|---------|-----------|-----------|----------------|\n",
    "| 1       | 1.0       | 2.0       | 2.5            |\n",
    "| 2       | 2.0       | 4.0       | 3.8            |\n",
    "| 3       | 3.0       | 6.0       | 5.9            |\n",
    "| 4       | 4.0       | 8.0       | 8.2            |\n",
    "| 5       | 5.0       | 10.0      | 10.1           |\n",
    "\n",
    "Compute the residuals:\n",
    "\n",
    "$$\n",
    "\\text{Residuals} = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Compute **MAE**:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{5} \\left( |2.0 - 2.5| + |4.0 - 3.8| + |6.0 - 5.9| + |8.0 - 8.2| + |10.0 - 10.1| \\right) = \\frac{1}{5} (0.5 + 0.2 + 0.1 + 0.2 + 0.1) = 0.22\n",
    "$$\n",
    "\n",
    "Compute **MSE**:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{5} \\left( (2.0 - 2.5)^2 + (4.0 - 3.8)^2 + (6.0 - 5.9)^2 + (8.0 - 8.2)^2 + (10.0 - 10.1)^2 \\right) = \\frac{1}{5} (0.25 + 0.04 + 0.01 + 0.04 + 0.01) = 0.07\n",
    "$$\n",
    "\n",
    "Compute **RMSE**:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\text{MSE} } = \\sqrt{0.07} \\approx 0.265\n",
    "$$\n",
    "\n",
    "Compute $R^2$:\n",
    "\n",
    "First, compute the total sum of squares:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{5} (2.0 + 4.0 + 6.0 + 8.0 + 10.0) = 6.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SS}_{\\text{tot}} = \\sum_{i=1}^{5} ( y_i - \\bar{y} )^2 = (2.0 - 6.0)^2 + (4.0 - 6.0)^2 + (6.0 - 6.0)^2 + (8.0 - 6.0)^2 + (10.0 - 6.0)^2 = 16 + 4 + 0 + 4 + 16 = 40\n",
    "$$\n",
    "\n",
    "Compute the residual sum of squares:\n",
    "\n",
    "$$\n",
    "\\text{SS}_{\\text{res}} = \\sum_{i=1}^{5} ( y_i - \\hat{y}_i )^2 = 0.25 + 0.04 + 0.01 + 0.04 + 0.01 = 0.35\n",
    "$$\n",
    "\n",
    "Compute $R^2$:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{ \\text{SS}_{\\text{res}} }{ \\text{SS}_{\\text{tot}} } = 1 - \\frac{0.35}{40} = 1 - 0.00875 = 0.99125\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- The model explains approximately 99.125% of the variance in the dependent variable.\n",
    "- The high $R^2$ value indicates a strong fit to the data.\n",
    "\n",
    "### Negative $R^2$ Values\n",
    "\n",
    "Although $R^2$ typically ranges from 0 to 1, it can be negative when the model is worse than simply predicting the mean of the observed data. This can happen if the residual sum of squares is greater than the total sum of squares.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Suppose we have a model that makes poor predictions resulting in a high residual sum of squares:\n",
    "\n",
    "- $\\text{SS}_{\\text{res}} = 50$\n",
    "- $\\text{SS}_{\\text{tot}} = 40$\n",
    "\n",
    "Compute $R^2$:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{50}{40} = 1 - 1.25 = -0.25\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- A negative $R^2$ indicates that the model performs worse than a horizontal line at $\\bar{y}$.\n",
    "- It suggests that the model fails to capture the underlying patterns in the data.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- For **model selection**, it is important to use multiple metrics to evaluate and compare models, as no single metric provides a complete assessment.\n",
    "- Be mindful of **outliers**, as they can disproportionately affect metrics like MSE and RMSE, potentially skewing the evaluation.\n",
    "- A very high **$R^2$** does not necessarily indicate that the model generalizes well to new data, so always check for **overfitting** by validating the model on unseen data.\n",
    "- Perform **residual analysis** by examining residual plots to ensure assumptions of linearity, homoscedasticity (constant variance), and normality of errors are met.\n",
    "- Use **adjusted $R^2$** when comparing models with different numbers of predictors, as it accounts for model complexity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
